{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "48b2a0c97dd95b4cc2df73196d4010a2",
     "grade": false,
     "grade_id": "cell-018df4a51b3570ec",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<img src=\"logo-2020.png\" alt=\"frankfurt school hmi\" style=\"width: 160px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f4a6b5a2ffd651b7b2dc628956a1dc9e",
     "grade": false,
     "grade_id": "cell-6312cd58e1a8d29a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Machine Learning I.\n",
    "## Problem Set 3:  Time Series Movement Prediction, Feature Engineering, and Model Comparison (14 points total)\n",
    "\n",
    "### Instructions\n",
    "The graded portion of problem set 1 consists of one notebook:\n",
    "```\n",
    "ps3_nb1-ml.ipynb\n",
    "```\n",
    "\n",
    "### Due Date\n",
    "* Group D: 04-MAR-2020 before 23:59:59 (CET)\n",
    "\n",
    "### Instructor\n",
    "* Prof. Dr. Gregory Wheeler ([gregorywheeler.org](http://gregorywheeler.org))\n",
    "\n",
    "---\n",
    "\n",
    "### Declare your collaborators\n",
    "You may work alone or in a group. The maximum group size is 4 people total. \n",
    "\n",
    "If you work in a group, use the next cell to enter the list of names (first, last) of your collaborators. \n",
    "~~~python\n",
    "# Example\n",
    "COLLABORATORS = ['Stu Dent', 'May Bee', 'Ki Val Storr']\n",
    "~~~\n",
    "You should also familiarize yourself with the collaboration policy on the course Canvas page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be sure your names are strings\n",
    "COLLABORATORS = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "835053984b8fdebb4e6c0622bc57e7b5",
     "grade": false,
     "grade_id": "cell-9e5bd09f3dfc27a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You have stock data for four semiconductor companies: <b>Analog Devices</b> (ADI), <b>Intel</b> (INTC), <b>Nvidia</b> (NVDA), and  <b>Xilinx</b> (XLNX). This assignment will walk through some basic exploratory data analysis (EDA), preprocessing data and feature engineering, and comparing the performance of two models.  This assignment is based on results reported by ([Basak, Kar, Saha, Khaidem, and Dey 2019](https://www.sciencedirect.com/science/article/abs/pii/S106294081730400X?via%3Dihub)).\n",
    "\n",
    "The assignment consists of seven parts arranged in three sections: \n",
    "\n",
    "- I. EDA\n",
    "    1. Working with Dictionaries\n",
    "    2. Plotting Time Series \n",
    "    3. Preliminary Analysis\n",
    "- II. Predicting Price Movement \n",
    "    4. Exponential Smoothing\n",
    "    5. Logistic Regression vs Random Forests\n",
    "    6. Feature Engineering with Technical Indicators \n",
    "- III. Analysis\n",
    "    7. Short Answer\n",
    "\n",
    "Each stock data set is a  `.csv` formatted file covering a twenty-year history of trading data between January 15, 2000 and January 15, 2020. The next cell loads each into a [pandas](http://pandas.pydata.org/pandas-docs/stable/) dataframe.  Following convention, we use the notation `df` to denote a pandas dataframe.  You of course can use the `type` command to identify an object's datatype. You may also find the command `whos` helpful to list what variables are currently stored in memory, their size, and type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# datetime allows you to format time stamps in time series data  \n",
    "import datetime as dt\n",
    "\n",
    "# load pandas dataframes\n",
    "df_adi = pd.read_csv('ADI.csv')\n",
    "df_intc = pd.read_csv('INTC.csv')\n",
    "df_nvda = pd.read_csv('NVDA.csv')\n",
    "df_xlnx = pd.read_csv('XLNX.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Exploratory Data Analysis \n",
    "Before starting a supervised or unsupervised machine learning project, a critical first step is to perform <b>exploratory data analysis</b> (<b>EDA</b>) to get a preliminary understanding of your data.  \n",
    "\n",
    "A first step is to use the `.head(5)` method to inspect the first 5 entries of your 3 data frames.  This code\n",
    "\n",
    "~~~python\n",
    "df_adi.head(5)\n",
    "~~~\n",
    "returns the first 5 entries of the dataframe `df_adi`. In addition to `.head()`, you may also wish to use other methods, such as `.describe()`, which gives you descriptive statistics about the values of each of your features (columns); `.shape` which gives you the dimensions of your array; `.info()`, which gives you the data types of each feature (column), a summary of all the data types in your dataframe, and how much memory is used.\n",
    "\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> For this assignment, you will need to write and test some code of your own.  Recall that you can add (+) or delete (scissors) highlighted cells from the toolbar:\n",
    "    \n",
    "<img src=\"ps3_fig1.png\" alt=\"ps3_fig1\" style=\"width: 100px;\"/>  \n",
    "\n",
    "Note that it will be important to <b>remove any substantive code</b> you write and <b>new code cells</b> you add before submitting the assignment.\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOF while scanning triple-quoted string literal (<ipython-input-3-8dbd24af9af0>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-8dbd24af9af0>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    df_adi.head(5)\u001b[0m\n\u001b[0m                  \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "df_adi.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_intc.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_nvda.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_xlnx.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_adi.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d7fdae85f446a81ca0039d63debddbd0",
     "grade": false,
     "grade_id": "cell-f48a46e9c29d6890",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1) Dictionaries\n",
    "Python [Dictionaries](https://docs.python.org/3/tutorial/datastructures.html#dictionaries) are a built-in data type that stores <b>key</b>-<b>value pairs</b>, just as an ordinary dictionary may be thought of as comprising a set of  word (key) and definition (value) pairs.  In ordinary dictionaries, words are arranged [lexicographically](https://en.wikipedia.org/wiki/Lexicographical_order)-- that is, in alphabetical order based on the letter components of the word.   \n",
    "\n",
    "For example, the following dictionary, `d_min`, has as keys the stock symbol names and as values the lowest adjusted closing price for each stock.\n",
    "\n",
    "~~~python\n",
    "d_min = {'ADI': 11.85, 'INTC': 8.58, 'NVDA': 2.26, 'XLNX': 9.88}  \n",
    "\n",
    "~~~\n",
    "[Associative array](https://en.wikipedia.org/wiki/Associative_array) is another name for a  dictionary. \n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>[[[ IMPORTANT NOTICE ]]]</b>: For technical reasons, JupyterHub runs Python 3.5.2 and Pandas 0.24.0.  One important implication of this is that, in Python 3.5.2 and Pandas 0.24.0, dictionaries are not order-preserving datatypes:  in other words, the order of the key-value pairs used when a dictionary is created is not preserved.   \n",
    "\n",
    "After [Python 3.6](https://docs.python.org/3/whatsnew/3.6.html#whatsnew36-compactdict), dictionaries <i>are</i> ordered datatypes: that is, after 3.6 dictionaries preserve the key-value item order. \n",
    "\n",
    "For those familiar with 3.6 or later releases of Python 3, you should keep this in mind.  You may find yourself writing some code for this assignment in which a specific ordering of dictionary items is necessary.  \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to create a dictionary [[[py 3.5.2]]]\n",
    "d_stocks = {'ADI': df_adi, 'INTC': df_intc, 'NVDA': df_nvda, 'XLNX': df_xlnx}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2)  Preliminary plots\n",
    "\n",
    "Once you have a bearing on the shape of your data, the number of features, their types (e.g., integers, floats, strings, lists, Booleans, dates, et cetera), the next step in EDA often involves plotting your raw data, when possible, to give you a rough understanding of how your data is distributed. \n",
    "\n",
    "The next block of code produces <b>three</b> plots for each stock: \n",
    "- (i) a raw data histogram plot for the <b>Trading Volume</b>, plotted in red; \n",
    "- (ii) the <b>Adjusted Closing Price</b>, plotted in blue; and \n",
    "- (iii) the <b>one-day percentage change</b> in closing price, plotted in magenta.  \n",
    "\n",
    "When run, you should see twelve subplots, arranged in a 3x4 grid.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Much of the code in the next cell is to do with creating the 3x4 grid of empty subplots and populating this grid with the right plots, iteratively, with `for` loops. It may help to see the stand-alone code to produce the upper-right plot, displaying the trading volume of ADI: \n",
    "\n",
    "\n",
    "~~~python\n",
    "## Plot the raw Volume column from the AMD dataframe\n",
    "df_adi['Volume'].plot(label='Volume', color='r', title = 'ADI', legend=True)\n",
    "# x-axis label\n",
    "plt.xlabel('Trading days: 2000-2020')\n",
    "# y-axis label\n",
    "plt.ylabel('Volume Traded')\n",
    "# show the plot\n",
    "plt.show()  \n",
    "~~~\n",
    "\n",
    "You may wish to run this block to see for yourself and modify it to recreate other plots in the grid if you are uncertain about how any part of the code works. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to produce raw plots \n",
    "\n",
    "# set width of plots\n",
    "ww = 20\n",
    "#set height of plots\n",
    "hh = 5\n",
    "\n",
    "# list of keys\n",
    "keys = list(d_stocks.keys()) \n",
    "\n",
    "#list of values\n",
    "values = list(d_stocks.values())\n",
    "\n",
    "#create and populate subplots\n",
    "plt.subplots_adjust(wspace=.2, hspace=.2)\n",
    "plt.figure(figsize=(ww, hh))\n",
    "\n",
    "# first row\n",
    "for i in range(len(keys)):\n",
    "    plt.subplot(1, len(keys), i+1)\n",
    "    values[i]['Volume'].plot(label='Volume', color='r', title = keys[i], legend=True)\n",
    "    #intc_df['Adj Close'].plot(label='INTC', legend=True)\n",
    "    plt.ylabel('Volume Traded')\n",
    "    plt.xlabel('Trading days: 2000-2020')\n",
    "    \n",
    "plt.show()  # show the plot\n",
    "#plt.clf()  # clear the plot space for the next plot\n",
    "\n",
    "# second row\n",
    "plt.figure(figsize=(ww, hh))\n",
    "for i in range(len(keys)):\n",
    "    plt.subplot(1, len(keys), i+1)\n",
    "    values[i]['Adj Close'].plot(label='Adj Close', legend=True)\n",
    "    plt.ylabel('Adjusted closing price')\n",
    "    plt.xlabel('Trading days: 2000-2020')\n",
    "    \n",
    "plt.show()  # show the plot\n",
    "#plt.clf()  # clear the plot space for the next plot\n",
    "\n",
    "# third row\n",
    "plt.figure(figsize=(ww, hh))\n",
    "for i in range(len(keys)):\n",
    "# Histogram of the daily price change percent of Adj_Close\n",
    "    plt.subplot(1, len(keys), i+1)\n",
    "    values[i]['Adj Close'].pct_change().plot.hist(bins=50, color='m', legend=True)\n",
    "    plt.xlabel('Adjusted close percent change in 1 day')\n",
    "    plt.ylabel('Frequency of change')\n",
    "    \n",
    "plt.show()  # show the plot\n",
    "plt.clf()  # clear the plot space for the next plot\n",
    "del i # clear the index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a37bb86049c6e51f2f048823d6d4087e",
     "grade": false,
     "grade_id": "cell-5225a6b8d2fb74af",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 3) Preliminary Analysis\n",
    "To answer the next three questions, you may find it useful to write a few lines of code that use some built-in pandas [methods](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) for working with dataframes, such as [.max()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.max.html#pandas.DataFrame.max), [.min()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.min.html#pandas.DataFrame.min), and perhaps even [idxmax](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.idxmax.html#pandas.DataFrame.idxmax) and [idxmin](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.idxmin.html#pandas.DataFrame.idxmin). For Question 3, you might find both [pct_change()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pct_change.html#pandas.DataFrame.pct_change) and [.var()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.var.html#pandas.DataFrame.var) useful.\n",
    "\n",
    "\n",
    "Both Question 1 and Question 2 require you to give a list and dictionary, whereas Question 3 is a multiple choice, multiple answer question. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "76fad9accf9dc3584cb6c7c8e0935f8c",
     "grade": false,
     "grade_id": "cell-08abf67de058566a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "___\n",
    "\n",
    "## Question 1\n",
    "\n",
    "What is the correct ordering of stocks by maximum volume, from highest to lowest? Complete the function `ans_one()` with your answer. Specifically, you need to find the maximum volume of each stock then order the stocks by their respective maximum trading volumes, from highest volume to lowest.  \n",
    "\n",
    "Provide two lines of code to specify the variables `d_list` and `d_max`: \n",
    "\n",
    "~~~python\n",
    "d_list =   # a list, ordered max volume to min volume\n",
    "d_max =   # a dictionary, ordered max volume to min volume\n",
    "~~~\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "max_volume_adi = df_adi['Volume'].max()\n",
    "max_volume_intc = df_intc['Volume'].max()\n",
    "max_volume_nvda = df_nvda['Volume'].max()\n",
    "max_volume_xlnx = df_xlnx['Volume'].max()\n",
    "list_max_volumes = [(\"ADI\", max_volume_adi), ('INTC', max_volume_intc), ('NVDA', max_volume_nvda), ('XLNX', max_volume_xlnx)]\n",
    "list2 = sorted(list_max_volumes, key=lambda z: z[1], reverse = True)\n",
    "d_list = [x[0] for x in list2]\n",
    "d_max = dict(list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "db0c4f8d6946f96cbc8dec893146e2a3",
     "grade": false,
     "grade_id": "ans_one",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def ans_one():\n",
    "    \"\"\" Returns a list of stocks ordered by volume\n",
    "        and a dictionary ordered by volume. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    :d_list:  list\n",
    "        The list of stock names ordered by volume,  max to min\n",
    "    :d_max:   dictionary\n",
    "        The dictionary of stocks ordered by volume, max to min\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    :d_list:\n",
    "    :d_max:\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    d_list = ['INTC', 'NVDA', 'XLNX', 'ADI']\n",
    "    d_max = {'ADI': 27748000, 'INTC': 309347600, 'NVDA': 230771400, 'XLNX': 67156700}\n",
    "    #raise NotImplementedError()\n",
    "    return d_list, d_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e2ebdb0864e6d505de24b6c0f3b16b98",
     "grade": true,
     "grade_id": "ans_one-answer",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "af1de76e74abe48fbc5dfc67cc8e988d",
     "grade": true,
     "grade_id": "ans_one-test1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test cell\n",
    "d_list, d_max = ans_one()\n",
    "if (type(d_list) == list and type(d_max) == dict):\n",
    "    assert True\n",
    "else: \n",
    "    print(\"ERROR: Check that `d_list` is a list, and `d_max` a dictionary\")\n",
    "    raise AssertionError(\"Type error: executing ans_one() should return a list and dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "80992f79502aaac59fb55c7351bad5f9",
     "grade": true,
     "grade_id": "ans_one-test2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "49de700b6abf0070e5f76cd853c79b98",
     "grade": false,
     "grade_id": "cell-7627b1cc15a66328",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "___\n",
    "\n",
    "## Question 2\n",
    "\n",
    "What is the correct ordering of stocks by their maximum Adjusted Closing Price, from highest to lowest? \n",
    "\n",
    "Complete the function `ans_two()` with your answer. Specifically, you need to provide two lines of code specify the variables `d_ans` and `d_max`: \n",
    "\n",
    "~~~python\n",
    "d_ans =   # a list, ordered Adjusted Closing Price, max to min\n",
    "d_max =   # a dictionary, Adjusted Closing Price, max to min\n",
    "~~~\n",
    "\n",
    "The grader will evaluate adjusted closing price rounded to two decimal places using Python's built-in [round](https://docs.python.org/3/library/functions.html#round) function. \n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "max_acp_adi = df_adi['Adj Close'].max()\n",
    "max_acp_intc = df_intc['Adj Close'].max()\n",
    "max_acp_nvda = df_nvda['Adj Close'].max()\n",
    "max_acp_xlnx = df_xlnx['Adj Close'].max()\n",
    "list_max_acp = [(\"ADI\", max_acp_adi), ('INTC', max_acp_intc), ('NVDA', max_acp_nvda), ('XLNX', max_acp_xlnx)]\n",
    "list2 = sorted(list_max_acp, key=lambda z: z[1], reverse = True)\n",
    "d_list = [x[0] for x in list2]\n",
    "d_max = dict(list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "561c3b128e441764deca3b6432ddc748",
     "grade": false,
     "grade_id": "ans_two",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def ans_two():\n",
    "    \"\"\" Returns a list of stocks ordered by Adjusted Closing Price\n",
    "        and a dictionary ordered by Adjusted Closing Price. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    :d_list:  list\n",
    "        The list of stock names ordered by Adj Close,  max to min\n",
    "    :d_max:   dictionary\n",
    "        The dictionary of stocks ordered by Adj Close, max to min\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    :d_list:\n",
    "    :d_max:\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    d_list = ['NVDA', 'XLNX', 'ADI', 'INTC']\n",
    "    d_max = {'ADI': 123.431526, 'INTC': 60.84, 'NVDA': 287.946198, 'XLNX': 138.247116}\n",
    "\n",
    "    #raise NotImplementedError()\n",
    "    return d_list, d_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aff8b7e032f41753cdc826d115ab15b3",
     "grade": true,
     "grade_id": "ans_two-answer",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5b8134b8595f4961646b4f00fefeb64e",
     "grade": true,
     "grade_id": "ans_two-test1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test cell\n",
    "d_list, d_max = ans_two()\n",
    "if (type(d_list) == list and type(d_max) == dict):\n",
    "    assert True\n",
    "else: \n",
    "    print(\"ERROR: Check that `d_list` is a list, and `d_max` a dictionary\")\n",
    "    raise AssertionError(\"Type error: executing ans_one() should return a list and dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7e19913d7000b8ae421383d58054f9a8",
     "grade": true,
     "grade_id": "ans_two-test2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Question 3\n",
    "\n",
    "Each stock has a peak price, $P_1$, which you calculated to answer Question 2. Following $P_1$, there is a minimum price or trough, $T$.  Following $T$ there may be another peak, $P_2$, called the <b>rebound price</b>. For purposes of the next question we say that the <b>rebound recovery percentage</b> <i>(RRP)</i> of a stock is calculated by \n",
    "\n",
    "$$ RRP := \\frac{P_2}{P_1} $$\n",
    "\n",
    "which is 1.00 if $P_2 = P_1$.\n",
    "\n",
    "Which of the following are true? Select all and only that apply.  \n",
    "\n",
    "* A) None of the stocks have seen their closing price drop more than 50% after their peak price. \n",
    "\n",
    "* B) ADI has the highest rebound recovery.\n",
    "\n",
    "* C) XLNX has the highest rebound recovery.\n",
    "\n",
    "* D) NVDA has the highest variance in one-day percentage price change.\n",
    "\n",
    "* E) INTC has the lowest variance in one-day percentage price change.\n",
    "\n",
    "* F) None of the above are true\n",
    "\n",
    "From the list of possible answers `['A', 'B', 'C', 'D', 'E', 'F']`, complete the next function with a list of your answer(s).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#for adi\n",
    "P1 = max_acp_adi\n",
    "rows_adi = df_adi.shape[0]\n",
    "index_max_adi = df_adi['Adj Close'].idxmax()\n",
    "T_index = df_adi.loc[index_max_adi + 1: rows_adi, 'Adj Close'].idxmin()\n",
    "T_value = df_adi.loc[index_max_adi + 1: rows_adi, 'Adj Close'].min()\n",
    "P2 = df_adi.loc[T_index + 1: rows_adi, 'Adj Close'].max()\n",
    "RRP_adi = P2 / P1\n",
    "#Pct_change = pct_change(P1, T_value)\n",
    "#Pct_change\n",
    "RRP_adi\n",
    "var_adi = df_adi['Adj Close'].var()\n",
    "a = (P1 - T_value) / P1\n",
    "rebound_recovery = P1 - P2\n",
    "rebound_recovery, RRP_adi, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#for intc\n",
    "P1 = max_acp_intc\n",
    "rows_intc = df_intc.shape[0]\n",
    "index_max_intc = df_intc['Adj Close'].idxmax()\n",
    "T_index = df_intc.loc[index_max_intc + 1: rows_intc, 'Adj Close'].idxmin()\n",
    "T_value = df_intc.loc[index_max_intc + 1: rows_intc, 'Adj Close'].min()\n",
    "P2 = df_intc.loc[T_index + 1: rows_intc, 'Adj Close'].max()\n",
    "RRP_intc = P2 / P1\n",
    "RRP_intc\n",
    "var_intc = df_intc['Adj Close'].var()\n",
    "var_intc\n",
    "a = (P1 - T_value) / P1\n",
    "rebound_recovery = P1 - P2\n",
    "rebound_recovery, RRP_intc, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#nvda\n",
    "P1 = max_acp_nvda\n",
    "rows_nvda = df_nvda.shape[0]\n",
    "index_max_nvda = df_nvda['Adj Close'].idxmax()\n",
    "T_index = df_nvda.loc[index_max_nvda + 1: rows_nvda, 'Adj Close'].idxmin()\n",
    "T_value = df_nvda.loc[index_max_nvda + 1: rows_nvda, 'Adj Close'].min()\n",
    "P2 = df_nvda.loc[T_index + 1: rows_nvda, 'Adj Close'].max()\n",
    "RRP_nvda = P2 / P1\n",
    "RRP_nvda\n",
    "var_nvda = df_nvda['Adj Close'].var()\n",
    "var_nvda\n",
    "a = (P1 - T_value) / P1\n",
    "rebound_recovery = P1-P2\n",
    "rebound_recovery, RRP_nvda, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#xlnx\n",
    "P1 = max_acp_xlnx\n",
    "rows_xlnx = df_xlnx.shape[0]\n",
    "index_max_xlnx = df_xlnx['Adj Close'].idxmax()\n",
    "T_index = df_xlnx.loc[index_max_xlnx + 1: rows_xlnx, 'Adj Close'].idxmin()\n",
    "T_index = df_xlnx.loc[index_max_xlnx + 1: rows_xlnx, 'Adj Close'].idxmin()\n",
    "P2 = df_xlnx.loc[T_index + 1: rows_xlnx, 'Adj Close'].max()\n",
    "RRP_xlnx = P2 / P1\n",
    "RRP_xlnx\n",
    "var_xlnx = df_xlnx['Adj Close'].var()\n",
    "var_xlnx\n",
    "a = (P1 - T_value) / P1\n",
    "rebound_recovery = P1 - P2\n",
    "rebound_recovery, RRP_xlnx, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1aaa7ba26d9f7222bdc605e185e861b8",
     "grade": false,
     "grade_id": "ans_three",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def ans_three():\n",
    "    \"\"\" Returns a list of your answers.  \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    :ans:  list\n",
    "        The list of your answers. Elements of :ans: are strings.\n",
    "    Returns\n",
    "    -------\n",
    "    :ans:\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    ans = ['D','E']\n",
    "    #raise NotImplementedError()\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f43b12e3655bce3fb765f3643ac20092",
     "grade": true,
     "grade_id": "ans_three-answer",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7dfccdb0eafc392e36b436c2eeea678e",
     "grade": true,
     "grade_id": "ans_three-test1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test cell\n",
    "ans = ans_three()\n",
    "possible_ans = ['A', 'B', 'C', 'D', 'E', 'F']\n",
    "if type(ans) == list and all(ii in possible_ans for ii in ans):\n",
    "    assert True\n",
    "else:\n",
    "    print(\"Error: Check that your answer, \" + repr(ans) + \", is admissible and the correct format.\")\n",
    "    raise AssertionError(\"Inadmissible answer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ab1a795c38a26d4532da1f887c45e2ed",
     "grade": false,
     "grade_id": "cell-75b3f0e52c58d4ed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# II. Predicting Movement\n",
    "\n",
    "In this section you will run two classification models on the smoothed data, then do some feature engineering and rerun the same two models on your stock data with those features.  \n",
    "\n",
    "Your task then is to compare the performance of predicting the movement of your stocks using a logistic regression model and a random forest model.\n",
    "\n",
    "## 4) Exponential Smoothing\n",
    "The first step is to preprocess data by <b>exponential smoothing</b>.  The smoothed statistic $s_t$ at time $t$ is an  function of the current observation, $x_t$, and previous observations $x_{t-1}, x_{t-2},\\ldots, x_0$, and the smoothing factor $\\alpha$: \n",
    "\n",
    "$$ s_t = \\frac{x_t + (1-\\alpha)x_{t-1}+(1-\\alpha)^2x_{t-2}+\\cdots+(1-\\alpha)^tx_0}{1+(1-\\alpha)+(1-\\alpha)^2 + \\cdots + (1-\\alpha)^t}$$\n",
    "\n",
    "This equation is the [adjusted version](https://pandas.pydata.org/pandas-docs/stable/user_guide/computation.html#exponentially-weighted-windows) of the exponential weighted window for finite histories, which is the default used by `emw()`.  The smoothing factor $\\alpha$ takes values between $0$ and $1$ (excluding 0), where it should be clear that $s_t = x_t$ if $\\alpha = 1$, which corresponds to no smoothing.\n",
    "\n",
    "The next cell perform some data preparation steps for you. We delete the feature `Date`, because we will simply use an integer index to preserve the time order, and delete the feature `Adj Close`. Movement in the opening price in `Open` will be the target variable we wish to predict.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d49e4cdcb5803150d6242094d659f66",
     "grade": false,
     "grade_id": "cell-379b1ca0aa2ab949",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# remove 'Date' and 'Adj Close' features from each stock dataframe \n",
    "for key, value in d_stocks.items():\n",
    "    # \n",
    "    del(value['Date'])\n",
    "    del(value['Adj Close'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "13dd2a04891f3ca89449cd66e472e3ac",
     "grade": false,
     "grade_id": "cell-cf10c59eaaabd853",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Your next task is to finish the function, `exp_smooth()`, using the pandas [.ewm method()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.ewm.html) and [.mean()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.mean.html)  to calculate <b>exponential weighted average</b> of a dataframe.  \n",
    "\n",
    "The `ewm()` method has a number of adjustable parameters, but for this task you should only use the smoothing factor $\\alpha$, which takes values between 0 and 1. To be explicit, your use of this method should be of the following form:\n",
    "\n",
    "~~~python\n",
    "df.ewm(alpha=alpha)\n",
    "~~~\n",
    "applied here to some data frame `df`.\n",
    "\n",
    "\n",
    "The `mean()` method also has a number of parameters, but you should not use any (i.e., leave the parentheses blank to use the default parameter values).\n",
    "\n",
    "For this task, you need to attend to two things.  First, to complete the function, you need to supply one line of code to compute exponential smoothing to `df`, a datframe, assigned to the local variable `smoothed_df`: \n",
    "\n",
    "~~~python\n",
    "\n",
    "smoothed_df =  ## finish your line of code here\n",
    "\n",
    "~~~\n",
    "\n",
    "Second, the smoothing factor default value (when one is not specified) is set to 1/2:\n",
    "\n",
    "~~~python\n",
    "alpha=0.5\n",
    "~~~\n",
    "\n",
    "Values for `alpha` (using a variable named `ta_alpha`, below) will be adjusted by you later in the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "242e05f0521d39353c05cfdb0ceab9e6",
     "grade": false,
     "grade_id": "expo_smooth",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def exp_smooth(df, alpha=0.5):\n",
    "    \"\"\" Returns the exponential weighted average of a dataframe \n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        :df: dataframe\n",
    "            The input dataframe \n",
    "        :alpha: float\n",
    "            The smoothing factor|\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        :smoothed_df: dataframe\n",
    "            The exponential weighted average of df with smoothing alpha\n",
    "    \"\"\" \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    smoothed_df = df.ewm(alpha = alpha).mean()\n",
    "    #raise NotImplementedError()\n",
    "    return smoothed_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b0bf2ec1dc613a3717687d3a455641d3",
     "grade": true,
     "grade_id": "expo_smooth-test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# public test condition: the exponential weighted average of the\n",
    "# Opening price of ADI on day 1000 with default alpha = 0.5 rounded\n",
    "# to 3 decimal places is 48.823:\n",
    "\n",
    "if round(exp_smooth(df_adi)['Open'][1000], 3) == 48.823:\n",
    "    assert True\n",
    "else:\n",
    "    print(\"Error: Check that your exp_smooth() function is implemented correctly.\")\n",
    "    raise AssertionError(\"Incorrect answer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2bad0265f460bb47c4dd06afcf20c1bf",
     "grade": false,
     "grade_id": "cell-53178eb2f02593f6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 5) Direction-Prediction with Logistic Regression and Random Forests\n",
    "\n",
    "In this part, you will construct two models that predict the direction of stock prices realizing a gain ($y = 1$) or loss ($y=0$). Specifically, you will compare the performance of a logistic regression classifier, which you have worked with already, to an ensemble of CART classification models, otherwise known as a <b>random forest</b> classifier.\n",
    "\n",
    "Predicting returns is typically posed as a forecasting regression problem, where prices are predicted. The problem may nevertheless be posed as a classification problem which predicts whether a stock price will increase or decrease.  The model you will develop here is adapted from an approach taken by ([Basak, Kar, Saha, Khaidem, and Dey 2019](https://www.sciencedirect.com/science/article/abs/pii/S106294081730400X?via%3Dihub)), and is markedly different than standard time-series methods:\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\"Standard models used in stock price forecasting involves statistical methods such as time series modeling and multivariate analysis ... where the stock price movements are usually treated as a function of time and solved as a regression problem. Conversely, in this paper as we pose it as a classification problem, the class label of each sample is determined by considering the t-day return. In our analysis, we have conducted experiments on t = 3, 5, 10, 15, 30, 60, and 90 days. The goal is to design an intelligent model that learns from the market data using machine learning techniques and predicts the direction in which a stock price will change at the closing time everyday. The ability to forecast direction of stock prices for individuals and companies capable of holding on to investments over medium to long-run should be a very useful support to this literature\" (Basak et al. 2019, p. 553).\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cb156090c819d214fa0ff60ff5b0163c",
     "grade": false,
     "grade_id": "cell-b9eee5946df3c7f0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "From this point on, you will be modifying your stock data frames, by adding new columns (features), and later, for step 6, smoothing the original data with different values of the smoothing factor $\\alpha$.  The next code cell collects together some code to be run all at once under a particular value for alpha in an effort to help ensure that smoothing is <i>only applied once</i>. \n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> Because you are making modifications to your datasets, it is a good idea to clear and rerun the notebook as you complete each task to make sure your code runs without errors from the start.</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy stock dataframes\n",
    "adi = df_adi\n",
    "intc = df_intc\n",
    "nvda = df_nvda\n",
    "xlnx = df_xlnx\n",
    "\n",
    "# Stock dictionary used for smoothing and feature engineering.\n",
    "s_stock_d = {'ADI': adi, 'INTC': intc, 'NVDA': nvda, 'XLNX': xlnx}\n",
    "\n",
    "# smoothing factor\n",
    "alpha = .5\n",
    "\n",
    "# apply smoothing to all stocks in dict\n",
    "for key, value in s_stock_d.items():\n",
    "        s_stock_d[key] = exp_smooth(value,alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your next task is to compare the closing price from $n$-days ago to the current closing price. If the $n$-day ago price is greater than or equal to the current price, then `pred = 1`; otherwise, `pred = 0`.  \n",
    "\n",
    "To effect this comparison, you may wish to use the [.shift(-n)](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shift.html), where $n$ is an integer used to specify the number of trading days the index will be shifted.  For example, you may wish to compare\n",
    "\n",
    "~~~python\n",
    "adi.shift(-1)['Close']\n",
    "~~~\n",
    "to\n",
    "~~~python\n",
    "adi['Close']\n",
    "~~~\n",
    "\n",
    "to see the output.\n",
    "\n",
    "Your task is to write one line of code to produce a Series consisting of a column of 1's and 0's, saved to the local variable `pred`, that will serve as your vector of classification labels. In other words, `pred` stores $y^{(i)} = 1$ or $y^{(i)} = 0$ for each day.\n",
    "\n",
    "~~~python\n",
    "    pred =  #YOUR CODE HERE    # Hint: returns 0 if a loss\n",
    "    pred = pred.iloc[:-n]\n",
    "    return pred.astype(int)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "641e80d6a9d3e57516418717b4634195",
     "grade": false,
     "grade_id": "pred_int",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def prediction_int(df, n):\n",
    "    \"\"\" Returns an integer label for n day closing price losses/gains.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    :df:     dataframe\n",
    "        A stock dataframe\n",
    "    :n:      int\n",
    "        The prediction horizon in days\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    :data:   series\n",
    "        The single column of integer predictions: 0 is a predicted loss, 1 otherwise.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pred = pd.Series(df.shift(-n)['Close'] >= df['Close'], dtype='int32')\n",
    "    #raise NotImplementedError()\n",
    "    pred = pred.iloc[:-n]\n",
    "    return pred.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ef081d8c817945b7b77236ba5aa3ade3",
     "grade": true,
     "grade_id": "pred_int_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test cell \n",
    "if prediction_int(intc,8)[265] == 1:\n",
    "    assert True\n",
    "else:\n",
    "    raise AssertionError(\"The function predict_int is not implemented correctly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, days):\n",
    "    \"\"\" Adds prediction vector and prepares data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    :df:     dataframe\n",
    "        A stock dataframe\n",
    "    :days:   int\n",
    "        The prediction horizon in days\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    :data:   dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    data = df.dropna().iloc[:-days]\n",
    "    data['pred'] = prediction_int(data, n=days)\n",
    "    del(data['Close'])\n",
    "    return data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cbf4fdb802d2a59d0dfeaf4634ce1713",
     "grade": false,
     "grade_id": "cell-9a40ff48d11288ed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Here is a block of code that operates on a single data frame `df = s_stock_d['ADI']` and a particular time window `days = 7` to produce a target vector of 1s and 0s, `y = data['pred']`, and the features of the `data` without labels, `X = data.drop('pred',axis =1)`:\n",
    "\n",
    "~~~python\n",
    "#prepare ADI stock data with 7 day forecast window\n",
    "data = prepare_data(s_stock_d['ADI'], 7)\n",
    "\n",
    "# create target label vector\n",
    "y = data['pred']\n",
    "\n",
    "# create features by removing the target labels\n",
    "X = data.drop('pred',axis =1)\n",
    "\n",
    "~~~\n",
    "You will be asked, below, to evaluate <i>all</i> stocks. So, it will be more efficient to write some code using [iterators](https://wiki.python.org/moin/Iterator) to generalize this preparation step to prepare all stocks in the dictionary `s_stocks_d`. The next cell should be used to write complete such a function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4ae416437d5c7ea5f40b46bd9d1e15aa",
     "grade": false,
     "grade_id": "features-targets",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def features_targets_dict(d, days):\n",
    "    \"\"\" Prepares a dictionary that stores the target Series y \n",
    "        and features Dataframe X with a prediction window (days)\n",
    "        for dictionary of stock Dataframes (d)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    :d:     dictionary\n",
    "        A dictionary of stocks whose values are dataframes\n",
    "    :days: int\n",
    "        The prediction horizon in days\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    :datasets:    dictionary of labaled datasets for each stock\n",
    "    :targetsets:  dictionary of target label vectors for each stock\n",
    "    :featuresets: dictionary of unlabeled datasets for each stock\n",
    "    \"\"\"\n",
    "\n",
    "    # dictionary data prep\n",
    "    datasets = {}\n",
    "    targetsets = {}\n",
    "    featuresets = {}\n",
    "    for k in d.keys():\n",
    "        datasets[k] = prepare_data(d[k],days)\n",
    "        # YOUR CODE HERE\n",
    "        targetsets[k] = datasets.get(k)['pred']\n",
    "        featuresets[k] = datasets.get(k).drop('pred', axis = 1)\n",
    "        #raise NotImplementedError()\n",
    "    return datasets, targetsets, featuresets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fdf34de28ed84b1efcb1965c28eac366",
     "grade": false,
     "grade_id": "cell-9cac750ec63a0f1c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Important:</b> The next test cells for <b>features_targets_dict()</b> assume that <b>alpha = 0.5</b>. The public test conditions will produce an error if you preprocess your data with a different value for alpha.\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f1080c080e3e3c041a64d9f0a5183ff8",
     "grade": true,
     "grade_id": "features-targets-test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test cell\n",
    "# features_targets_dict() test\n",
    "datasets, targetsets, featuresets = features_targets_dict(s_stock_d, 7)\n",
    "if round(datasets['ADI']['Open'][55], 3) == 74.289  and round(datasets['NVDA']['Open'][555], 3) == 13.884:\n",
    "    assert True\n",
    "else:\n",
    "    print(\"Hint: check that you are iterating over all the stock dataframes\")\n",
    "    raise AssertionError(\"Error in features_targets_dict()\")\n",
    "\n",
    "if targetsets['ADI'].isnull().values.any() == False:\n",
    "    assert True\n",
    "else:\n",
    "    print(\"Your targetset contains NaNs. Hint: Try using the .dropna() method.\")\n",
    "    raise AssertionError(\"Error in targetsets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a6357d32bd302082784a88beb6ff6e06",
     "grade": false,
     "grade_id": "cell-f17b72800f2fc5f6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### SK-Learn\n",
    "Recall the workflow logic and API format for sklearn.  To review, briefly, once libriaries are imported, you should prepare your data, split into test and training sets, fit a model, perform predictions on a test set, then evaluate the results with appropriate numerical performance measures.\n",
    "\n",
    "To help you, here is some code that will run on a <i>single dataframe</i>.  Given the questions you will be asked to consider, you will want to adapt this to iterate over all items of your stock dictionary, and perhaps even iterate over the models.  Ideally, you should collect together the results of your model in a single dataframe that stores  all the results in one table.  What you choose to do, however, is up to you.\n",
    "\n",
    "### Sample code for producing a Random Forest Model.\n",
    "\n",
    "\n",
    "<img src=\"ps3_fig4.png\" alt=\"frankfurt school hmi\" style=\"width: 900px;\"/>\n",
    "\n",
    "\n",
    "### Important for Your Implementation:\n",
    " - For a Logistic Regression model, replace \n",
    "\n",
    "~~~python\n",
    "clf = RandomForestClassifier(n_jobs=-1, n_estimators=65, random_state=212)\n",
    "~~~\n",
    "with\n",
    "~~~python\n",
    "clf = LogisticRegression()\n",
    "~~~\n",
    "\n",
    "#### Note:\n",
    " - The exact parameters for `RandomForestClassifier` (i.e., `n_jobs = -1`, `n_estimators = 65`, `random_state = 212`), must be used.  You should not specify any parameters for `LogisticRegression()`. \n",
    "\n",
    " - A 70/30 split must be used: it is strongly recommended to incorporate the following line of code to ensure a standardized split.\n",
    " \n",
    "~~~python\n",
    "train_set_split = int(len(X) * 0.7) \n",
    "~~~\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Important:</b> To ensure standardized answers, you <b>must</b> use these parameter settings and only these settings.  In addition, we are continuing to use a 7-day window and `alpha = 0.5`.\n",
    "</div> \n",
    "\n",
    "The next cell loads models and metrics from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, confusion_matrix, recall_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using sklearn, you should build relevant models to answer the next and subsequent questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "lrmodel = LogisticRegression()\n",
    "rfmodel = RandomForestClassifier(n_jobs = -1, n_estimators = 65, random_state = 212)\n",
    "models = [lrmodel, rfmodel]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "featuresets.keys()\n",
    "report = pd.DataFrame(columns=['Model','Stock Name', 'Precision', 'recall', 'f1 score', 'accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def tts(targetsets, featuresets, models):\n",
    "    for m in models:\n",
    "        for k in targetsets.keys():\n",
    "            # YOUR CODE HERE\n",
    "            train_test_split = int(len(featuresets.get(k)) * 0.7) \n",
    "            X_train = featuresets.get(k)[:train_test_split]\n",
    "            X_test = featuresets.get(k)[train_test_split:]\n",
    "            y_train = targetsets.get(k)[:train_test_split]\n",
    "            y_test = targetsets.get(k)[train_test_split:]\n",
    "            report = run_model(m, X_train, X_test, y_train, y_test, k)\n",
    "    return report\n",
    "    #return targetsets, featuresets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#A function to do all steps for each model and stock data\n",
    "def run_model(model, X_train, X_test, y_train, y_test, k):\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    precision = precision_score(y_pred = pred, y_true = y_test)\n",
    "    recall = recall_score(y_pred = pred, y_true = y_test)\n",
    "    f1 = f1_score(y_pred = pred, y_true = y_test)\n",
    "    accuracy = accuracy_score(y_pred = pred, y_true = y_test)\n",
    "    model_name = type(model).__name__\n",
    "    report.loc[len(report)] = [model_name, k, precision, recall, f1, accuracy]\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "tts(targetsets, featuresets, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "round(report['accuracy'], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for m in report['accuracy']:\n",
    "    print(round(m,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "d_acc = {'ADI LR': 0.564,'INTC LR': 0.425 , 'NVDA LR': 0.609, 'XLNX LR': 0.543, 'ADI RF': 0.430,'INTC RF': 0.495, 'NVDA RF': 0.416, 'XLNX RF': 0.509}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "40f1087a5a14531e7e791e318ccf6d38",
     "grade": false,
     "grade_id": "cell-de3ee481a3b17847",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "## Question 4\n",
    "\n",
    "There are four stocks (ADI, INTC, NVDA, XLNX) and two classification models (Logistic Regression, Random Forest). Pre-process your data using a 7-day window and smoothing factor $\\alpha = 0.5$. What is the ranking of the accuracy results (rounded to 3 decimal places) of the resulting 8 models, from best to worst?  \n",
    "\n",
    "To answer this question, you will need to supply a dictionary whose <b>keys</b> are from the following list:\n",
    "\n",
    "~~~python\n",
    "# keys for the dictionary returned by ans_four()\n",
    "['ADI LR','INTC LR', 'NVDA LR', 'XLNX LR', 'ADI RF','INTC RF', 'NVDA RF', 'XLNX RF']  \n",
    "~~~\n",
    "and whose <b>values</b> are the accuracy scores of each model, rounded to 3 decimal places.  To be clear, 'LR' denotes a logistic regression model and 'RF' a random forest model.  Thus, within the dictionary `d_acc` returned by `ans_four()`, `'ADI LR'` is the dictionary key value used to refer to the accuracy score of a logistic regression classification model for the ADI stock. Your answer to Question 4 will be two lines of code for `ans_four()`, one for the dictionary `d_acc` and another for the ordered list of stock names `ordered_stocks`. Each line has the following generic forms:\n",
    "\n",
    "~~~python\n",
    "\n",
    "d_ans = {'Key_1': 0.000, 'Key_2': 0.000, 'Key_3': 0.000, 'Key_4': 0.000,\n",
    "         'Key_5': 0.000, 'Key_6': 0.000, 'Key_7': 0.000, 'Key_8': 0.000}\n",
    "\n",
    "ordered_stocks = ['Key_of_highest','Key_2nd', 'Key_3rd', 'Key_4th','Key_5th',\n",
    "                  'Key_6th', 'Key_7th', 'Key_of_lowest']\n",
    "         \n",
    "~~~\n",
    "\n",
    "To avoid mistyped answers, you may wish to copy-and-paste each model's key name from the list of keys above.   To ensure your dictionary <b>values</b> pass the tests correctly, <b>your accuracy scores</b> that you enter in `d_ans` should be rounded to 3 decimal places using  \n",
    "\n",
    "~~~python\n",
    "# code that will be used to test your key values. \n",
    "round(acc, 3) \n",
    "~~~\n",
    "wherel `acc` is the accuracy score for the corresponding model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f26df4c29286040d6d902b99202978d3",
     "grade": false,
     "grade_id": "ans_four",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def ans_four():\n",
    "    \"\"\" Returns a dictionary ordered by accuracy. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    :d_acc:   dictionary\n",
    "        The dictionary of stocks and thier accuracy score to 3 decimal places\n",
    "    \n",
    "    :ordered_stocks:   list\n",
    "        The list of string names ordered by their accuracy score to 3 decimal places, max to min\n",
    "    Returns\n",
    "    -------\n",
    "    :d_acc:\n",
    "    :ordered_stocks:\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    d_acc = {'ADI LR': 0.564,'INTC LR': 0.425 , 'NVDA LR': 0.609, 'XLNX LR': 0.543, \n",
    "             'ADI RF': 0.430,'INTC RF': 0.495, 'NVDA RF': 0.416, 'XLNX RF': 0.509}\n",
    "    ordered_stocks = ['NVDA LR', 'ADI LR', 'XLNX LR', 'XLNX RF', 'INTC RF', 'ADI RF', 'INTC LR','NVDA RF']\n",
    "    #raise NotImplementedError()\n",
    "    return d_acc, ordered_stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f1aed31c4e8cc82253c60ce32fbe918b",
     "grade": true,
     "grade_id": "ans_four-answer",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c78cd0e09c6229d1374f6bfb97eddfbf",
     "grade": true,
     "grade_id": "ans_four-test-1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test cell\n",
    "d_acc, ordered_stocks = ans_four()\n",
    "if (type(d_acc) == dict and len(d_acc) == 8):\n",
    "    assert True\n",
    "else: \n",
    "    print(\"ERROR: Check that `d_acc` is a dictionary with 8 items.\")\n",
    "    raise AssertionError(\"Error: ans_four()\")\n",
    "\n",
    "if (type(ordered_stocks) == list and len(ordered_stocks) == 8):\n",
    "    assert True\n",
    "else: \n",
    "    print(\"ERROR: Check that `ordered_stocks` is a list with 8 items and correctly ordered\")\n",
    "    raise AssertionError(\"Error: ans_four()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1b2580013a36d7d391f29a62f6fd3649",
     "grade": false,
     "grade_id": "cell-9b68d5d85ff752b5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "## Question 5\n",
    "Multiple choice questions. Use the same model parameters as for Question 4: a 7-day window and smoothing factor $\\alpha = 0.5$. \n",
    "\n",
    "Select all and only that are true.\n",
    "\n",
    " - A) A random forest model produces the lowest F1 score\n",
    " - B) NVDA Logistic Regression has the highest Accuracy, Precision, Recall, and F1 Score.  In other words, there is no other stock whose scores are strictly greater than NVDA's scores.  \n",
    " - C) All random forest models have a precision score greater than 0.5.\n",
    " - D) No random forest model has an F1 score greater than 0.5.\n",
    " - E) NVDA is the stock whose F1 score is the most sensitive to model choice (i.e., the stock whose F1 scores differ the <b>most</b> wrt Logistic Regression and Random Forests given the parameter settings.)  \n",
    " - F) XLNX is the stock whose Precision score is the least sensitive to model choice (i.e., the stock whose Precision scores differ the <b>least</b> wrt Logistic Regression and Random Forests given the parameter settings.\n",
    "\n",
    "From the list of possible answers `['A', 'B', 'C', 'D', 'E', 'F']`, complete the next function with a list of your answer(s).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5ba7a04f5c882b48f43c04f53165c8c9",
     "grade": false,
     "grade_id": "ans_five",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def ans_five():\n",
    "    \"\"\" Returns a list of your answers.  \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    :ans:  list\n",
    "        The list of your answers. Elements of :ans: are strings.\n",
    "    Returns\n",
    "    -------\n",
    "    :ans:\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    ans = ['B', 'D', 'E', 'F']\n",
    "    #raise NotImplementedError()\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dabbe04f9fc6a923d28313c6f9a24e56",
     "grade": true,
     "grade_id": "ans_five-test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test cell\n",
    "ans = ans_five()\n",
    "possible_ans = ['A', 'B', 'C', 'D', 'E', 'F']\n",
    "if type(ans) == list and all(ii in possible_ans for ii in ans):\n",
    "    assert True\n",
    "else:\n",
    "    print(\"Error: Check that your answer, \" + repr(ans) + \", is admissible and the correct format.\")\n",
    "    raise AssertionError(\"Inadmissible answer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6a583e022a1ce7fca604b064c492e78e",
     "grade": false,
     "grade_id": "cell-0bf1bfdcbd3896c8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 6) Feature Engineering with Technical Indicators\n",
    "\n",
    "In this section of the assignment you will prepare new features for your stock data using a range of technical indicators.  The next cell imports a library of technical indicators, followed by a brief description of those you will use to add features to your stock dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# technical indicators\n",
    "import pandas_technical_indicators as ta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5ede1a3b3427a3413f3ea6040a4a6e38",
     "grade": false,
     "grade_id": "cell-61ec2305119d922b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Technical Indicators \n",
    "Technical indicators use price and volume information to identify statistical trends from historical trading activity to predict future movements in price or volume.  Common indicators used to forecast price movement include:\n",
    "\n",
    "(1) <b>Relative strength index ([RSI](https://www.investopedia.com/terms/r/rsi.asp))</b>,  (Wilder 1978): is a  momentum  indicator that compares bullish or bearish price momentum of an asset to its price.  The RSI oscillates between 0 and 100, where values of 70 or greater indicates  overbought conditions and values of 30 or less indicate underbought conditions. \n",
    "\n",
    "(2) <b>Stochastic Oscillator  ([OS](https://www.investopedia.com/terms/s/stochasticoscillator.asp))</b>, (Lane 1984): is a  momentum  indicator  that compares a security's closing price to a historical window of past prices.  The range of OS values is between 0 and 100.\n",
    "\n",
    "(3) <b>Williams percentage range ([%R](https://www.investopedia.com/terms/w/williamsr.asp))</b>, (Williams 1978): is a  momentum  indicator that indicates the level of a closing price in relation to the highest price for stock within a 14-day look-back period. \n",
    "\n",
    "(4) <b>Moving average convergence divivergence ([MACD](https://www.investopedia.com/terms/m/macd.asp))</b>, (Appel 2005): is a  momentum  indicator that compares the 26-day exponential moving average (EMA) to the 12-day EMA.  The MCAD is the difference between the 26-day EMA and 12-day EMA.\n",
    "\n",
    "(5) <b>Price rate of change ([PROC](https://www.investopedia.com/terms/p/pricerateofchange.asp))</b>, (Larson 2015): is a momentum-based oscillator that measures the percentage change between the current price and the price over a period of time.  In finance the acronym 'ROC' is used.  However, in machine learning, 'ROC' refers to [receiver operating characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) curve. We use PROC here to avoid ambiguity.\n",
    "\n",
    "(6) <b>On balance volume ([OBV](https://www.investopedia.com/terms/o/onbalancevolume.asp))</b>, (Granville 1963): is a momentum indicator that uses the flow of volume to estimate changes in price.  The indicator keeps a running total of trading volume, accumulating volume on days when the price increases, subtracts volume on days when prices decrease.\n",
    "\n",
    "(7) <b>Accumulation distribution line ([ADI](https://www.investopedia.com/terms/a/accumulationdistribution.asp))</b>, (Chaikin 2002): detects divergences between price and volume flow to predict the strength of a trend. \n",
    "\n",
    "(8) <b>Money Flow Index ([MFI](https://www.investopedia.com/terms/m/mfi.asp))</b>  (Quong and Soudack 1989) This is a  volume-weighted RSI, similar to the ADI in using volume flow and price to identify overbought or oversold conditions.\n",
    "\n",
    "(9) <b>Average True Range ([ATR](https://www.investopedia.com/terms/a/atr.asp))</b> (Wilder 1978): measures market volitility, typically over a 14 day moving average.\n",
    "\n",
    "\n",
    "(10) <b>Triple Exponential Average ([TREMA or TRIX](https://www.investopedia.com/articles/technical/02/092402.asp))</b>, (Hutson 1983) takes multiple exponential moving averages to identify short-term changes in price direction.\n",
    "\n",
    "(11) <b>Vortex Indicator ([VI](https://www.investopedia.com/terms/v/vortex-indicator-vi.asp))</b> (Botes and Siepman 2010) is designed to identify trend reversals and to confirm a current trend. \n",
    "\n",
    "(12) <b>Ease of Movement ([EoM or EVM](https://www.investopedia.com/terms/e/easeofmovement.asp))</b> (Arms 1999): is a volume-weighted momentum indicator that uses price volitility and volume to assess the \"ease\" versus \"resistance\" of price movement to discern how easily a price can move up or down.  \n",
    "\n",
    "(13) <b>Commodity Channel Index ([CCI](https://www.investopedia.com/terms/c/commoditychannelindex.asp))</b> (Lambert 1980): is a momentum indicator used to assess price trend and direction. \n",
    "\n",
    "---\n",
    "\n",
    "The next cell is a function, called `technical_features()`, that computes several technical indicators for a range of time windows (5, 14, 26, 44, and 66 day periods) and adds these computed values as feature columns to a stock dataframe.   \n",
    "\n",
    "This function is complete: you only need to run it. However, it is recommended that you study `technical_features()` to understand what it is doing for you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def technical_features(df):\n",
    "    \"\"\" adds technical features from pandas_technical_indicators \n",
    "        to a dataframe for 5, 14, 26, 44, and 66 day periods\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    :df:  pandas DataFrame\n",
    "        a data frame of stocks\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    :df:  pandas DataFrame\n",
    "        the dataframe df with new features and without Open, High, Low, and Volume\n",
    "    \"\"\"\n",
    "    for period in [5, 14, 26, 44, 66]:\n",
    "        df = ta.relative_strength_index(df, n=period)   #rsi\n",
    "        df = ta.stochastic_oscillator_d(df, n=period)   #so\n",
    "        df = ta.accumulation_distribution(df, n=period) #adl\n",
    "        df = ta.average_true_range(df, n=period)        #atr\n",
    "        df = ta.momentum(df, n=period)                  #mom\n",
    "        df = ta.money_flow_index(df, n=period)          #mfi\n",
    "        df = ta.rate_of_change(df, n=period)            #proc or roc\n",
    "        df = ta.on_balance_volume(df, n=period)         #obv\n",
    "        df = ta.commodity_channel_index(df, n=period)   #cci\n",
    "        df = ta.ease_of_movement(df, n=period)          #eom\n",
    "        df = ta.trix(df, n=period)                      #trix\n",
    "        df = ta.vortex_indicator(df, n=period)          #vi\n",
    "    \n",
    "    df = ta.macd(df, n_fast=12, n_slow=26)              #macd\n",
    "    \n",
    "    df['ema50'] = df['Close'] / df['Close'].ewm(50).mean()   #ema 50 days\n",
    "    df['ema21'] = df['Close'] / df['Close'].ewm(21).mean()   #ema 21 days\n",
    "    df['ema14'] = df['Close'] / df['Close'].ewm(14).mean()   #ema 14 days\n",
    "    df['ema5'] = df['Close'] / df['Close'].ewm(5).mean()     #ema  5 days\n",
    "    \n",
    "    # remove \n",
    "    del(df['Open'])\n",
    "    del(df['High'])\n",
    "    del(df['Low'])\n",
    "    del(df['Volume'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e4539229d02d6eb66f2e7a1d7aa09911",
     "grade": false,
     "grade_id": "cell-ddc6ba2e8b703b67",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The next cell modifies the `prepare_data()` function, from above, to add the technical indicator features to each stock dataframe using `technical_features()`.\n",
    "\n",
    "It is recommended that you study `ta_prepare_data()` to understand what it is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ta_prepare_data(df, days):\n",
    "    \"\"\" Adds prediction vector and prepares data with technical features\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    :df:     dataframe\n",
    "        A stock dataframe\n",
    "    :days:   int\n",
    "        The prediction horizon in days\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    :data:   dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    data = technical_features(df).dropna().iloc[:-days]\n",
    "    data['pred'] = prediction_int(data, n=days)\n",
    "    del(data['Close'])\n",
    "    return data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6ef526ab8f0883d0cbb9bc058c31e882",
     "grade": false,
     "grade_id": "cell-734da0f5b6bddb74",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<b>Your next task</b> is to complete the function `ta_features_targets_dict()`, which modifies a dictionary `d` of stock dataframes by adding the technical features to each stock dataframe in `d`, and returning three dictionarys: `datasets`, a dictionary of labeled datasets for each stock; `targetsets`, a dictionry of target label vectors for each stock; `featuresets`, a dictionary of unlabeled datasets for each stock.  The code for computing `datasets` is complete, but the code for `targetsets` and `featuresets` is missing.  Your task is to write two lines of code, one for `targetsets` and the other for `featuresets`.  There should be no `NaN` or \"missing values\" in either dictionary. \n",
    "\n",
    "You may find the pandas methods [drop()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html) and [dropna()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html?highlight=dropna)  helpful. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1f434114c0b69082e338f7eaa7036452",
     "grade": false,
     "grade_id": "prepare_2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def ta_features_targets_dict(d, days):\n",
    "    \"\"\" Prepares a dictionary that stores the target Series y \n",
    "        and technical features to Dataframe X with a prediction \n",
    "        window (days) for dictionary of stock Dataframes (d)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    :d:     dictionary\n",
    "        A dictionary of stocks whose values are dataframes\n",
    "    :days: int\n",
    "        The prediction horizon in days\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    :datasets:    dictionary of labaled datasets for each stock\n",
    "    :targetsets:  dictionary of target label vectors for each stock\n",
    "    :featuresets: dictionary of unlabeled datasets for each stock\n",
    "    \"\"\"\n",
    "\n",
    "    # dictionary data prep\n",
    "    datasets = {}\n",
    "    targetsets = {}\n",
    "    featuresets = {}\n",
    "    for k in d.keys():\n",
    "        datasets[k] = ta_prepare_data(d[k],days)\n",
    "        # YOUR CODE HERE\n",
    "        targetsets[k] = datasets.get(k)['pred']\n",
    "        featuresets[k] = datasets.get(k).drop('pred', axis = 1)\n",
    "        #raise NotImplementedError()\n",
    "    return datasets, targetsets, featuresets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ee4a7288a123a4a90e2b60deb32aa824",
     "grade": false,
     "grade_id": "cell-03e5979604a692f3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Adjusting Alpha\n",
    "In the next cell, you will have an opportunity to change the values of $\\alpha$.  Because it is important in the first half of the assignment that `alpha = 0.5`, you will adjust a parameter called `ta_alpha`.  Specifically, in the next cell you will see\n",
    "\n",
    "~~~python\n",
    "ta_alpha = 0.2\n",
    "~~~\n",
    "which can be adjusted to take values between 0 and 1, 0 excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'exp_smooth' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-4b1e4bdcc701>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# apply smoothing to all stocks in dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms_stock_d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0ms_stock_d\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_smooth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mta_alpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'exp_smooth' is not defined"
     ]
    }
   ],
   "source": [
    "# Copy stock dataframes\n",
    "adi = df_adi\n",
    "intc = df_intc\n",
    "nvda = df_nvda\n",
    "xlnx = df_xlnx\n",
    "\n",
    "# Stock dictionary used for smoothing and feature engineering.\n",
    "s_stock_d = {'ADI': adi, 'INTC': intc, 'NVDA': nvda, 'XLNX': xlnx}\n",
    "\n",
    "# adjust alpha here: values should be greater than 0 and less than 1\n",
    "ta_alpha = 0.6\n",
    "\n",
    "# apply smoothing to all stocks in dict\n",
    "for key, value in s_stock_d.items():\n",
    "        s_stock_d[key] = exp_smooth(value,ta_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3abdde58de33ffe503b6b8304aa58f9b",
     "grade": false,
     "grade_id": "cell-a8baa54ec48c6b57",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The next cell plots a comparison between unsmoothed and smoothed data sets to help guide intuitions about the effect of different values of the smoothing factor, $\\alpha$.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ww = 20\n",
    "hh = 5\n",
    "\n",
    "s_keys = list(s_stock_d.keys())\n",
    "s_values = list(s_stock_d.values())\n",
    "\n",
    "plt.subplots_adjust(wspace=.2, hspace=.2)\n",
    "plt.figure(figsize=(ww, hh))\n",
    "for i in range(len(s_keys)):\n",
    "    plt.subplot(1, len(keys), i+1)\n",
    "    values[i]['Open'].plot(label='Open', title = keys[i], legend=True)\n",
    "    plt.ylabel('Open Price')\n",
    "    plt.xlabel('Trading days: 2000-2020')\n",
    "    \n",
    "plt.subplots_adjust(wspace=.2, hspace=.2)\n",
    "plt.figure(figsize=(ww, hh))\n",
    "for i in range(len(keys)):\n",
    "# Plot the Adj Close columns \n",
    "    plt.subplot(1, len(keys), i+1)\n",
    "    s_values[i]['Open'].plot(label='Open Smoothed', title = keys[i], legend=True)\n",
    "    plt.ylabel('Open Price')\n",
    "    plt.xlabel('Trading days: 2000-2020')\n",
    "    \n",
    "plt.show()  # show the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b8ea5cd8a7dbac45da5e9136c66be719",
     "grade": true,
     "grade_id": "answer_ta",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'datasets_7, targetsets_7, featuresets_7 = ta_features_targets_dict(s_stock_d, 7)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'datasets_10, targetsets_10, featuresets_10 = ta_features_targets_dict(s_stock_d, 10)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'datasets_14, targetsets_14, featuresets_14 = ta_features_targets_dict(s_stock_d, 14)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'datasets_21, targetsets_21, featuresets_21 = ta_features_targets_dict(s_stock_d, 21)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, confusion_matrix, recall_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "lrmodel = LogisticRegression()\n",
    "rfmodel = RandomForestClassifier(n_jobs = -1, n_estimators = 65, random_state = 212)\n",
    "models = [lrmodel, rfmodel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "featuresets.keys()\n",
    "report = pd.DataFrame(columns=['Model','Stock Name', 'Precision', 'recall', 'f1 score', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#A function to do all steps for each model and stock data\n",
    "def run_model(model, X_train, X_test, y_train, y_test, k):\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    precision = precision_score(y_pred = pred, y_true = y_test)\n",
    "    recall = recall_score(y_pred = pred, y_true = y_test)\n",
    "    f1 = f1_score(y_pred = pred, y_true = y_test)\n",
    "    accuracy = accuracy_score(y_pred = pred, y_true = y_test)\n",
    "    model_name = type(model).__name__\n",
    "    report.loc[len(report)] = [model_name, k, precision, recall, f1, accuracy]\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def tts(targetsets, featuresets, models):\n",
    "    for m in models:\n",
    "        for k in targetsets.keys():\n",
    "            # YOUR CODE HERE\n",
    "            train_set_split = int(len(featuresets.get(k)) * 0.7) \n",
    "            X_train = featuresets.get(k)[:train_set_split]\n",
    "            X_test = featuresets.get(k)[train_set_split:]\n",
    "            y_train = targetsets.get(k)[:train_set_split]\n",
    "            y_test = targetsets.get(k)[train_set_split:]\n",
    "            report = run_model(m, X_train, X_test, y_train, y_test, k)\n",
    "    return report\n",
    "    #return targetsets, featuresets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "tts(targetsets_7, featuresets_7, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "tts(targetsets_10, featuresets_10, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "tts(targetsets_14, featuresets_14, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "tts(targetsets_21, featuresets_21, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "LR7 = report['f1 score'].iloc[0:4].sum()\n",
    "RF7 = report['f1 score'].iloc[4:8].sum()\n",
    "diff7 = LR7 - RF7\n",
    "LR10 = report['f1 score'].iloc[8:12].sum()\n",
    "RF10 = report['f1 score'].iloc[12:16].sum()\n",
    "diff10 = LR10 - RF10\n",
    "LR14 = report['f1 score'].iloc[16:20].sum()\n",
    "RF14 = report['f1 score'].iloc[20:24].sum()\n",
    "diff14 = LR14 - RF14\n",
    "LR21 = report['f1 score'].iloc[24:28].sum()\n",
    "RF21 = report['f1 score'].iloc[28:32].sum()\n",
    "diff21 = LR21 - RF21\n",
    "diff7, diff10, diff14, diff21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "LR7 = report['accuracy'].iloc[0:4].sum()\n",
    "RF7 = report['accuracy'].iloc[4:8].sum()\n",
    "diff7 = LR7 - RF7\n",
    "LR10 = report['accuracy'].iloc[8:12].sum()\n",
    "RF10 = report['accuracy'].iloc[12:16].sum()\n",
    "diff10 = LR10 - RF10\n",
    "LR14 = report['accuracy'].iloc[16:20].sum()\n",
    "RF14 = report['accuracy'].iloc[20:24].sum()\n",
    "diff14 = LR14 - RF14\n",
    "LR21 = report['accuracy'].iloc[24:28].sum()\n",
    "RF21 = report['accuracy'].iloc[28:32].sum()\n",
    "diff21 = LR21 - RF21\n",
    "diff7, diff10, diff14, diff21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Question 6\n",
    "Multiple choice questions. Select all and only that are true.\n",
    "\n",
    " - A) All things considered, a smaller alpha generally improves accuracy scores for all stocks.\n",
    " - B) For $\\alpha = 0.2$ (i.e., `ta_alpha = 0.2`), the F1 score for each model with a 7-day window is greater than the corresponding model's F1 score with a 14-day window.\n",
    " - C) For some $\\alpha$, all random forest models have a precision score greater than 0.5 in 7-day, 14-day, and 21-day window horizons.\n",
    " - D) For some $\\alpha$, no logistic regression model has an accuracy score above 0.5 in a 10-day window horizon.\n",
    " - E) NVDA is the stock whose F1 score is the most sensitive to model choice for 10-day window models with alpha = 0.5 (i.e., NVDA is the stock whose F1 scores differ the <b>most</b> wrt Logistic Regression and Random Forests given a 7-day, 10-day, 14-day, and 21-day horizon window  with `ta_alpha = 0.5`).  \n",
    " - F) The maximum difference in F1 score between Logistic Regression and Random Forests decreases as the n-day window horizon increases, for n = 7, 10, 14, 21, all compared with a constant alpha of 0.5 (that is, `ta_model = 0.5`).\n",
    "\n",
    "From the list of possible answers `['A', 'B', 'C', 'D', 'E', 'F']`, complete the next function with a list of your answer(s).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d9382c60ebde660d8975a4bcb9395e38",
     "grade": false,
     "grade_id": "ans_six",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def ans_six():\n",
    "    \"\"\" Returns a list of your answers.  \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    :ans:  list\n",
    "        The list of your answers. Elements of :ans: are strings.\n",
    "    Returns\n",
    "    -------\n",
    "    :ans:\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    ans = ['A', 'C', 'F']\n",
    "    #raise NotImplementedError()\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0aa86a94bf570a943a9eb2addcd9af1b",
     "grade": true,
     "grade_id": "ans_six-test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test cell\n",
    "ans = ans_six()\n",
    "possible_ans = ['A', 'B', 'C', 'D', 'E', 'F']\n",
    "if type(ans) == list and all(ii in possible_ans for ii in ans):\n",
    "    assert True\n",
    "else:\n",
    "    print(\"Error: Check that your answer, \" + repr(ans) + \", is admissible and the correct format.\")\n",
    "    raise AssertionError(\"Inadmissible answer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "01e4365ab4a6fae3c6398e7041e8eb6a",
     "grade": false,
     "grade_id": "cell-c575e86e0f24cb0b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "## Question 7\n",
    "\n",
    "There are 8 data points. Each observation is labeled either P, for positive class ($y =1)$, or N, for negative class ($y=0)$. Suppose that you have a binary classifier that produces a probability estimate for each observation belonging to the positive class. The eight data points are then ordered, from highest probability of that data point belonging to the positive class to lowest probability, yielding the following ordering: \n",
    "\n",
    "$$\\mathrm{PNPPNNPN}$$\n",
    "\n",
    "There are several ways to divide our ordered list of observations into two groups, those the model predicts are in the positive class and those the model predicts are all in the negative class.  This decision boundary may be determined by the probability threshold, which we symbolize by $\\wedge$.\n",
    "\n",
    "Which threshold or thresholds yields a precision of 1/2?  Select all and only that are true.\n",
    "\n",
    "  - A) $\\mathrm{PN_{\\wedge}PPNNPN}$\n",
    "  - B) $\\mathrm{PNP_{\\wedge}PNNPN}$\n",
    "  - C) $\\mathrm{PNPP_{\\wedge}NNPN}$\n",
    "  - D) $\\mathrm{PNPPN_{\\wedge}NPN}$\n",
    "  - E) $\\mathrm{PNPPNN_{\\wedge}PN}$\n",
    "  - F) $\\mathrm{PNPPNNP_{\\wedge}N}$\n",
    "  \n",
    "Given your answer, which is true?\n",
    "\n",
    "  - G) Recall is guaranteed to be greater than 1/2.\n",
    "  - H) Recall is guaranteed to be less than 1/2.\n",
    "  - I) Recall may be greater than or less than 1/2.\n",
    "  \n",
    "From the list of possible answers `['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I']`, complete the next function with a list of your answer(s).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "34efab0685e8874725369378eb4bf256",
     "grade": false,
     "grade_id": "ans_seven",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def ans_seven():\n",
    "    \"\"\" Returns a list of your answers.  \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    :ans:  list\n",
    "        The list of your answers. Elements of :ans: are strings.\n",
    "    Returns\n",
    "    -------\n",
    "    :ans:\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    ans = ['A', 'E', 'I']\n",
    "    #raise NotImplementedError()\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d99c299f7c3de80f4f0f6e9c19ccbab6",
     "grade": true,
     "grade_id": "ans_seven-test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test cell\n",
    "ans = ans_seven()\n",
    "possible_ans = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I']\n",
    "if type(ans) == list and all(ii in possible_ans for ii in ans):\n",
    "    assert True\n",
    "else:\n",
    "    print(\"Error: Check that your answer, \" + repr(ans) + \", is admissible and the correct format.\")\n",
    "    raise AssertionError(\"Inadmissible answer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Analysis\n",
    "\n",
    "\n",
    "The core of the model you developed is taken from ([Basak, Kar, Saha, Khaidem, and Dey 2019](https://www.sciencedirect.com/science/article/abs/pii/S106294081730400X?via%3Dihub)), where they state:\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\"The focus of the current paper is ... to implement random forests, and gradient boosted trees on stock data, and to discuss its advantages over non-ensemble techniques\" (Basak et al. 2019, p. 554).\n",
    "</div>\n",
    "\n",
    "A key step in their approach is the use of exponential smoothing as a preprocessing step, and you investigated the effect of using different values for the smoothing factor $\\alpha$ for this preprocessing step in the assignment. \n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\"In the current work, we have used exponential smoothing to remove random local variation in the data. A detailed survey of the existing literature does not offer this as a preprocessing step to earlier predictions\"  (p.558) </div>\n",
    " \n",
    "Basak et al. report very accurate results, and the argue that the good performance of their methodology is due primarily to ensemble methods, like random forests, working better on non-linear stock data than linear (metric) classifiers, like logistic regression. \n",
    "\n",
    "<div class=\"alert alert-block alert-success\">   \n",
    "\"The current methodology performs better than linear classifiers and the reason for this is the inherent non-linearity in the data. Our learning model to [sic] surpass all these metric classifiers in terms of long term prediction\" (pp. 558-559).\n",
    "</div>\n",
    "\n",
    "## 7) Short Answer (2 points)\n",
    "Do you agree with their analysis? If so, state why. If not, state why not.  \n",
    "\n",
    "You should limit your answer to one paragraph (~100 to 200 words).  Your answer should be correct, concise, well-reasoned, and clearly written.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1660496c35a9c512693a4ad1d7002ca6",
     "grade": true,
     "grade_id": "Short_Answer",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "As we increased the number of features in part 6, both logistic regression and Random Forest performed better. \n",
    "No matter what the smoothing factor is, we can see that Logisctic Regression is producion better results (F1 score and Accuracy) on avreage as compared to Random Forests for days = 7, 10, 14, 21. Alpha of 0.5 is producing the best scores for both Logistic Regression and Random forest because we generated optiomal data points for our smoothed plot. Because of the reason i stated here, i would disagree with their analysis. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d1b5e518b4f36fb3ac3af94067fb866a",
     "grade": true,
     "grade_id": "cell-fa682398a02dbc2d",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test for Collaborator policy before submission\n",
    "%run -i 'collaboration_test.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1000fcd8d6fde56e9fb229fd4ec15c44",
     "grade": false,
     "grade_id": "cell-915d1cbf713a4ccc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Before submitting this notebook, you should do the following steps:\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "    0. <b>Delete Any New Code You Introduced</b>: You are likely to have written some code that is computationally expensive, taking many seconds or even some minutes to run.  If so, your notebook is very likely to <b>fail</b> the <b>second validation</b> test if you submit the assignment without first removing all new substantive code blocks introduced by you.  What's substantive?  If you see: $$\\mathtt{In}\\ [ * ]:$$ next to a running cell for more than a few seconds.\n",
    "</div>\n",
    "  \n",
    "  1. __Restart Kernel__ (Kernel  Restart and Clear Output)\n",
    "  2. __Run all Cells__ (Cell  Run All)\n",
    "  3. __Validate__: Press the 'Validate' button above.\n",
    "  4. __Save File__ (File  Save and Checkpoint)\n",
    "  5. __Close and Shutdown Kernel__ (File  Close and Halt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
